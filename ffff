from typing import Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pymongo.collection import Collection
from datetime import datetime

from app.db.mongo_ops import MongoOperations
from app.model.enums import StepName, StepStatus
from app.utils.logger import setup_logger

logger = setup_logger(__name__)

class BlobUploadService:
    def upload_documents(
        self,
        tenant_id: str,
        engagement_id: str,
        year: int,
        quarter: str,
        request_id: str,
        db
    ) -> None:
        """
        Uploads all documents belonging to doc_run_ids for given request_id.
        Updates blob meta data in MongoDB.
        """
        run_docs_col: Collection = db["run_documents"]
        log_col: Collection = db["run_request_logs"]

        try:
            container_name = f"{tenant_id.lower()}-{engagement_id.lower()}"
            doc_run_ids = MongoOperations.get_doc_run_ids_from_logs(
                log_col,
                request_id=request_id,
                engagement_id=engagement_id
            )

            with ThreadPoolExecutor(max_workers=5) as executer:
                futures = []

                for doc_run_id in doc_run_ids:
                    documents = MongoOperations.get_documents_by_doc_run_id_and_request_id(
                        run_docs_col, doc_run_id, request_id
                    )

                    for doc in documents:
                        if doc.get("base64") == "LARGE_FILE":
                            logger.info(
                                f"[SKIPPED] Skipping blob upload for large file: {doc.get('original_filename')} | doc_id: {doc.get('doc_id')}"
                            )
                            continue

                        futures.append(
                            executer.submit(
                                self._upload_single_document,
                                run_docs_col,
                                container_name,
                                doc,
                                request_id
                            )
                        )

                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.exception(f"[THREAD ERROR] Exception during blob upload thread: {str(e)}")

            self._mark_blob_step_complete(log_col, request_id)
            logger.info(f"[Blob Upload Complete] All eligible files uploaded for request_id: {request_id}")

        except Exception as e:
            logger.exception(f"[FATAL] Failed to upload documents for request_id: {request_id} | Error: {str(e)}")

    def _mark_blob_step_complete(self, log_col: Collection, request_id: str) -> None:
        """
        Mark the blob upload step as complete in the log_col collection.
        """
        try:
            log_col.update_one(
                filter={"request_id": request_id},
                update={
                    "$set": {
                        "steps.$[elem].status": StepStatus.COMPLETED,
                        "steps.$[elem].timestamp": datetime.utcnow().isoformat(),
                        "steps.$[elem].result": "SUCCESS"
                    }
                },
                array_filters=[{"elem.step_name": StepName.BLOB_UPLOAD}]
            )
            logger.info(f"[STEP COMPLETE] Blob upload step marked complete for request_id: {request_id}")
        except Exception as e:
            logger.exception(f"[LOG ERROR] Failed to update ingestion log for request_id: {request_id} | Error: {str(e)}")
