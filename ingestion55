# app/services/ingestion_service.py

import uuid
import hashlib
from datetime import datetime
from typing import List, Dict, Any

from app.model.document_schema import DocumentModel, SharePointInfoModel, StepModel
from app.model.enums import StepName, StepStatus, StepResult, IngestionStatus
from app.utils.logger import setup_logger
from app.blob.blob_client import BlobUploader

logger = setup_logger(__name__)


class SharePointIngestionService:
    """
    Handles the ingestion of documents from SharePoint into MongoDB,
    applies deduplication using SHA256, stores base64 only for small files,
    and uploads large files directly to blob storage.
    """

    def __init__(self, db, client_id, client_secret):
        self.db = db
        self.client_id = client_id
        self.client_secret = client_secret
        self.run_management = db["run_management"]
        self.run_documents = db["run_documents"]
        self.sharepoint_logs = db["run_request_logs"]
        self.blob_uploader = BlobUploader()

    def ingest(self, sp_client, tenant_id: str, engagement_id: str, year: str, quarter: str,
               request_id: str, use_case_name: str, is_open_source_pipeline: bool) -> Dict[str, Any]:
        """
        Orchestrates SharePoint ingestion: retrieves metadata, deduplicates documents,
        handles large file logic, updates logs and DB status.
        """
        try:
            logger.info(f"[INGEST] Starting ingestion for request_id={request_id}, year={year}, quarter={quarter}")
            run_doc = self._fetch_run_metadata(tenant_id, engagement_id, request_id)
            if not run_doc:
                return {"status": StepResult.FAILED, "message": "Run management not found", "document_uploaded": False}

            run_id = run_doc.get("run_id")
            if not run_id:
                return {"status": StepResult.FAILED, "message": "Run ID not found", "document_uploaded": False}

            matched_meta = self._get_metadata_for_quarter(run_doc, year, quarter, request_id)
            if not matched_meta:
                return {"status": StepResult.FAILED, "message": "Metadata not found", "document_uploaded": False}

            all_docs, doc_run_ids = self._collect_documents(
                sp_client, matched_meta, request_id, run_id, tenant_id, engagement_id,
                year, quarter, use_case_name, is_open_source_pipeline
            )

            tape_doc = self._extract_tape_file(
                sp_client, matched_meta, request_id, run_id, tenant_id,
                engagement_id, year, quarter, use_case_name, is_open_source_pipeline
            )
            if tape_doc:
                all_docs.append(tape_doc)
                doc_run_ids.append(tape_doc["doc_run_id"])

            if all_docs:
                try:
                    self.run_documents.insert_many(all_docs)
                except Exception as e:
                    logger.exception(f"[MONGO ERROR] Failed to insert documents into Mongo: {e}")
                    self._mark_step_failed(request_id, StepName.SHAREPOINT_INGESTION, str(e))
                    self._log_error(request_id, str(e))
                    return {"status": StepResult.FAILED, "message": "Mongo Insert Failed", "document_uploaded": False}

                self._mark_step_success(request_id, StepName.SHAREPOINT_INGESTION)
                self.sharepoint_logs.update_one(
                    {"request_id": request_id},
                    {"$set": {
                        "status": IngestionStatus.INGESTION_COMPLETE,
                        "doc_run_ids": list(set(doc_run_ids)),
                        "document_uploaded": True,
                        "total_new_files": len(all_docs),
                        "completed_at": datetime.utcnow().isoformat()
                    }}
                )
                return {"status": StepResult.SUCCESS, "message": "Ingestion completed", "document_uploaded": True}
            else:
                logger.warning(f"[INGESTION] No new files found to ingest for request_id={request_id}")
                self._mark_step_failed(request_id, StepName.SHAREPOINT_INGESTION, "No files to Ingest")
                self.sharepoint_logs.update_one(
                    {"request_id": request_id},
                    {"$set": {
                        "status": IngestionStatus.NO_NEW_FILES,
                        "message": "No new files to ingest",
                        "document_uploaded": False,
                        "completed_at": datetime.utcnow().isoformat()
                    }}
                )
                return {"status": StepResult.FAILED, "message": "No new files", "document_uploaded": False}

        except Exception as e:
            logger.exception(f"[INGEST ERROR] request_id={request_id}: {e}")
            self._mark_step_failed(request_id, StepName.SHAREPOINT_INGESTION, str(e))
            self._log_error(request_id, str(e))
            return {"status": StepResult.FAILED, "message": str(e), "document_uploaded": False}

    def _collect_documents(self, sp_client, matched_meta: Dict[str, Any], request_id: str, run_id: str,
                           tenant_id: str, engagement_id: str, year: str, quarter: str,
                           use_case_name: str, is_open_source_pipeline: bool) -> (List[Dict[str, Any]], List[str]):
        """
        Collects SharePoint documents, applies SHA256 deduplication,
        stores base64 for small files, uploads large ones directly to blob.
        """
        all_docs = []
        seen_hashes = set()
        all_doc_run_ids = []
        year_quarter_path = matched_meta.get("year_quarter_path")

        if not year_quarter_path:
            self._log_error(request_id, "Missing year_quarter_path in metadata")
            return [], []

        for sub_product in matched_meta.get("sub_product_types", []):
            for obligor in sub_product.get("obligors", []):
                doc_run_id = obligor.get("doc_run_id")
                obligor_name = obligor.get("obligor_name")
                folder_path = f"{year_quarter_path}/{sub_product['sub_product_type']}/{obligor_name}"

                try:
                    folder = sp_client.get_folder(folder_path)
                    files = sp_client.traverse_files(folder)

                    for file_obj in files:
                        filename = file_obj.properties["Name"]
                        ext = filename.split(".")[-1].lower()

                        file_bytes = sp_client.get_file_bytes(file_obj)
                        sha256 = hashlib.sha256(file_bytes).hexdigest()

                        if sha256 in seen_hashes:
                            continue
                        seen_hashes.add(sha256)

                        base64_str = None
                        sas_url = None
                        blob_path = None

                        if len(file_bytes) < 16 * 1024 * 1024:
                            base64_str = sp_client.get_base64_content(file_obj)
                        else:
                            blob_path, sas_url = self.blob_uploader.upload_large_file(
                                file_bytes, filename, tenant_id, year, quarter, doc_run_id
                            )

                        doc = DocumentModel(
                            request_id=request_id,
                            doc_run_id=doc_run_id,
                            run_id=run_id,
                            tenant_id=tenant_id,
                            engagement_id=engagement_id,
                            year=int(year),
                            quarter=quarter,
                            sub_product_type=sub_product['sub_product_type'],
                            obligor_name=obligor_name,
                            doc_id=str(uuid.uuid4()),
                            original_filename=filename,
                            file_type=ext,
                            sha256=sha256,
                            base64=base64_str,
                            blob_path=blob_path,
                            sas_url=sas_url,
                            use_case_name=use_case_name,
                            is_open_source_pipeline=is_open_source_pipeline,
                            sharepoint_info=SharePointInfoModel(
                                relative_path=file_obj.properties["ServerRelativeUrl"],
                                library_name="Documents",
                                site_url=sp_client.site_url
                            ),
                            steps=[StepModel(
                                step_name=StepName.SHAREPOINT_INGESTION,
                                result=StepResult.SUCCESS,
                                status=StepStatus.COMPLETED,
                                timestamp=datetime.utcnow().isoformat()
                            )]
                        )

                        all_docs.append(doc.dict())
                        all_doc_run_ids.append(doc_run_id)

                except Exception as e:
                    logger.exception(f"[COLLECT ERROR] folder: {folder_path} — {e}")
                    continue

        return all_docs, list(set(all_doc_run_ids))

    def _extract_tape_file(self, sp_client, matched_meta, request_id, run_id, tenant_id,
                           engagement_id, year, quarter, use_case_name, is_open_source_pipeline):
        """
        Stub for Excel tape file (handled same as other documents).
        Can apply same SHA256 + base64 conditional logic.
        """
        # Your existing logic to extract Excel tape file goes here.
        return None

    def _fetch_run_metadata(self, tenant_id: str, engagement_id: str, request_id: str) -> Any:
        run_doc = self.run_management.find_one({"tenant_id": tenant_id, "engagement_id": engagement_id})
        if not run_doc:
            self._log_error(request_id, "Run management entry not found")
        return run_doc

    def _get_metadata_for_quarter(self, run_doc: Dict[str, Any], year: str, quarter: str, request_id: str) -> Any:
        matched = next((m for m in run_doc.get("metadata", []) if m["year"] == int(year) and m["quarter"] == quarter), None)
        if not matched:
            self._log_error(request_id, "No metadata for given year/quarter")
        return matched

    def _mark_step_success(self, request_id: str, step: StepName):
        logger.info(f"[STEP SUCCESS] {step.value} — request_id={request_id}")
        self.sharepoint_logs.update_one(
            {"request_id": request_id},
            {"$push": {
                "steps": {
                    "step_name": step,
                    "status": StepStatus.COMPLETED,
                    "result": StepResult.SUCCESS,
                    "timestamp": datetime.utcnow().isoformat()
                }
            }}
        )

    def _mark_step_failed(self, request_id: str, step: StepName, message: str):
        logger.error(f"[STEP FAILED] {step.value} — request_id={request_id} — {message}")
        self.sharepoint_logs.update_one(
            {"request_id": request_id},
            {"$push": {
                "steps": {
                    "step_name": step,
                    "status": StepStatus.FAILED,
                    "result": StepResult.FAILED,
                    "timestamp": datetime.utcnow().isoformat(),
                    "message": message
                }
            }}
        )

    def _log_error(self, request_id: str, message: str):
        logger.error(f"[ERROR] request_id={request_id} — {message}")
        self.sharepoint_logs.update_one(
            {"request_id": request_id},
            {"$set": {
                "status": IngestionStatus.ERROR,
                "message": message,
                "completed_at": datetime.utcnow().isoformat()
            }}
        )
